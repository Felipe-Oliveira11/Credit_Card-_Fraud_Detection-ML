{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detecção de Fraude.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1K4lTaDjJo4IHrPt9cEBlGQcmVIGSTDdb",
      "authorship_tag": "ABX9TyOELrb7GaWEBJGFZztzrUoF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Felipe-Oliveira11/Fraud-Detection-ML/blob/master/Detec%C3%A7%C3%A3o_de_Fraude.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Fetqy3aeeSa",
        "colab_type": "text"
      },
      "source": [
        "# Detecção de Fraude de Cartões \n",
        "\n",
        "\n",
        "Neste projeto vamos aplicar Machine learning para Detecção de fraude em Cartões de crédito, um problema muito grande que instituições financeiras e Fintechs tem diariamente, que é identificar se uma transação é uma fraude ou não, é uma tarefa díficil e extremamente delicada. É de extrema importância que as empresas emissoras de cartão de crédito estejam preparadas para este tipo de crime, monitorando constantemente o comportamento dos cartões. \n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUXHOdq9ejUL",
        "colab_type": "text"
      },
      "source": [
        "<p align=center>\n",
        "<img src=\"https://miro.medium.com/max/1000/0*_6WEDnZubsQfTMlY.png\" width=\"70%\"></p>\n",
        "\n",
        "https://ai-journey.com/wp-content/uploads/2019/06/fraud-EMV-chip-credit-card.jpg\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D0aAa55tXwN",
        "colab_type": "text"
      },
      "source": [
        "Muitas empresas já tem investido fortemente na Inteligência Artificial, para solucionar problemas financeiros e também criar produtos que minimizam riscos e maximizam lucros. \n",
        "\n",
        "Há diversos cases na indústria, sobre a importância que tem a Inteligência Artificial para as empresas que lidam com dados financeiros, que necessitam de escalabilidade e velocidade em aprovações de transações feitas por cartões de crédito, é um desafio muito maior que apenas criar um modelo de Machine learning, mas também permitir um alto desempenho dessas plataformas. \n",
        "\n",
        "\n",
        "Com o crescimento da necessidade, de se ter um sistema robusto que consiga lhe dizer se uma compra foi ou não fraude, as oportunidades de se aplicar Inteligência Artificial são cada vez mais necessárias, e vem se tornando imprescindível fazer uso da IA para combater um problema tão grande.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvQ5rqNxwsW2",
        "colab_type": "text"
      },
      "source": [
        "<p align=center>\n",
        "<img src=\"https://news.mit.edu/sites/mit.edu.newsoffice/files/styles/news_article_image_top_slideshow/public/images/2018/MIT-Fraud-Detection-PRESS_0.jpg?itok=laiU-5nR\" width=\"60%\"></p>\n",
        "\n",
        "\n",
        "https://www.eastwestbank.com/ReachFurther/NewsArticleStore/519/Credit-card-fraud-top.jpg\n",
        "\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-NXj3wYx93p",
        "colab_type": "text"
      },
      "source": [
        "### Solução "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2usZFkEejZn",
        "colab_type": "text"
      },
      "source": [
        "O objetivo é fazer o uso do Machine learning para antecipar uma fraude, reduzindo posteriormente os números de fraudes que acabam se concretizando e que não são identificadas a tempo, é uma tarefa que vai exigir muitos experimentos, técnicas e algoritmos que melhor performam em cima dos dados que temos, além de o modelo deve ter o mesmo nível de performance quando inserido em produção.  \n",
        "\n",
        "<hr>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9XtGSq0bDgX",
        "colab_type": "text"
      },
      "source": [
        "### *Sobre* o dataset\n",
        "\n",
        "Os conjuntos de dados contêm transações realizadas com cartões de crédito em setembro de 2013 por portadores de cartões europeus.\n",
        "Este conjunto de dados <b>apresenta transações que ocorreram em dois dias </b>, nas quais temos 492 fraudes em 284.807 transações. O conjunto de dados é altamente desequilibrado, a classe positiva (fraudes) representa 0,172% de todas as transações.\n",
        "\n",
        "<br>\n",
        "\n",
        "Ele contém apenas variáveis ​​de entrada numéricas que são o resultado de uma transformação PCA. Infelizmente, devido a <b> problemas de confidencialidade </b>, não podemos fornecer os recursos originais e mais informações básicas sobre os dados.\n",
        "\n",
        "\n",
        "Features V1, V2,… V28 são os principais componentes obtidos com o PCA, as únicas features que não foram transformadas com o PCA são 'Time' (Tempo) e 'Amount' (Valor da transação).\n",
        "\n",
        "<br> \n",
        "\n",
        "\n",
        "* A feature 'Time'(Tempo) contém os segundos decorridos entre cada transação e a primeira transação no conjunto de dados.\n",
        "\n",
        "*  A feature 'Amount' é o valor da transação. \n",
        "\n",
        "* A feature 'Class' é a variável de resposta e assume o valor 1 em caso de fraude e 0 em caso contrário.\n",
        "\n",
        "<br>\n",
        "<b>\n",
        " 0: Transação normal  |\n",
        " 1: Transação fraudulenta\n",
        " </b>\n",
        "\n",
        "<br>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnL4PFmrfYz1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pacotes \n",
        "!pip install shap \n",
        "!pip install yellowbrick\n",
        "!pip install imblearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2e7sw3qa0gz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns \n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import NearMiss\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "from sklearn.metrics import classification_report, recall_score, precision_score ,average_precision_score, plot_precision_recall_curve\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from yellowbrick.classifier import PrecisionRecallCurve \n",
        "from sklearn.model_selection import train_test_split, cross_validate ,KFold, StratifiedShuffleSplit, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, PowerTransformer, QuantileTransformer, RobustScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier \n",
        "from lightgbm import LGBMClassifier\n",
        "import shap\n",
        "shap.initjs()\n",
        "\n",
        "%matplotlib inline \n",
        "from warnings import simplefilter\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgunQ_MPbDM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importando dados \n",
        "path = '/content/drive/My Drive/ML_Notebooks/Detecção de Fraude /creditcard.csv' \n",
        "credit = pd.read_csv(path)\n",
        "credit.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSpz6xW-e4Wp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Linhas: {} | Colunas: {} '.format(credit.shape[0], credit.shape[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reuklCRU0Ig2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Estatística descritiva\n",
        "credit.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NhpsaTu0NPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classes\n",
        "sns.countplot(x=credit['Class'], palette='coolwarm')\n",
        "print('Normal: {} |  Fraude: {}'.format(credit[credit['Class']==0].shape[0] , credit[credit['Class']==1].shape[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhnEFPhqi73R",
        "colab_type": "text"
      },
      "source": [
        "De inicío fica claro o desbalanceamento de classes, isso é o que torna a modelagem de Fraude tão delicada, um evento que é raro e que acontece de forma sútil, vamos criar hipóteses para fazer o Feature engineering com esses dados, afim de maximizar as métricas: <b> ROC AUC | Precision | Recall </b>.\n",
        "\n",
        "Tentar medir a performance do modelo com a Acurácia, seria um engano pois teriamos uma acurácia alta, o que de fato não resolveria o nosso problema, pois o conjunto possiu classes desbalanceadas, vamos focar nestas três métricas listadas acima, que não variam com o desbalanceamento das classes. \n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImeLpVyw2S4S",
        "colab_type": "text"
      },
      "source": [
        "### Limpeza de dados \n",
        "\n",
        "Antes de iniciar uma análise dos dados, vamos limpar os dados, não será necessário apagar transações duplicadas pois faz total sentido, conter transaçõe que são feitas rotineiramente.  \n",
        "\n",
        "* Missing values \n",
        "* Ruídos \n",
        "* Tipo de dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c74e-HzY2Lev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def missing_values(data):\n",
        "\n",
        "    \"\"\" Resumo de dados nulos\n",
        "        contidos no dataset \"\"\"\n",
        "\n",
        "    # total de nulos     \n",
        "    missing = data.isnull().sum()\n",
        "    total = missing.sort_values(ascending=True)\n",
        "    \n",
        "    # porcentagem \n",
        "    percent = (missing / len(data.index ) * 100).round(2).sort_values(ascending=True)\n",
        "\n",
        "    # concatenação \n",
        "    table_missing = pd.concat([total, percent], axis=1, keys=['Números de NA', 'Porcentagem de NA'])\n",
        "\n",
        "    return table_missing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA5qpEEK4Ijx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "missing_values(credit)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M2OLGW58Mtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tipo de dados\n",
        "credit.dtypes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-lWpmR48nCA",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuBQzdNW8sVz",
        "colab_type": "text"
      },
      "source": [
        "### Análise de dados\n",
        "\n",
        "Nesta etapa vou tentar identificar alguns padrões, já tenho em mente que vou fazer alguns experimentos tratando os Outliers posteriormente, criar alguns modelos e comparar as aborgadens feitas. \n",
        "\n",
        "<hr>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNyYkBI4HOtL",
        "colab_type": "text"
      },
      "source": [
        "Vou checar a distribuição de segundos, quero identificar o perído de tempo, em que uma transação fraudulenta é feita, comparando com as transações normais. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKIKj1l2FMLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Comparar Distribuições\n",
        "\n",
        "plt.figure(figsize=(16,12))\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.title('Segundos Transação Normal')\n",
        "sns.distplot(credit[credit['Class']==0]['Time'], color='purple')\n",
        "\n",
        "print('\\n')\n",
        "print('\\n')\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.title('Segundos Transação Fraudulenta')\n",
        "sns.distplot(credit[credit['Class']==1]['Time'], color='red')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUWnHxp2LNbj",
        "colab_type": "text"
      },
      "source": [
        "Olhando para as duas Distribuições, conseguimos ver que o período de uma transação rotulada como Normal, ocorre em uma média de uma forma trivial o que é considerado algo normal, já as transações fraudulentas ocorrem em um período de tempo maior, acada <b> 50.000 segundos </b> uma nova transação fraudulenta é feita(<b> Em dois dias de transações </b>).É muito importante identificar que é uma distribuição muito próxima da <b> Uniforme </b> pois tem uma curva de densidade, quase no mesmo ângulo constante.\n",
        "\n",
        "\n",
        "Precisamos verificar posteriormente, se ocorre uma transação fraudulenta mais de  uma vez, com os mesmos dados da transação. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I99Y9DNMhGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  Fraudes repetidas ?\n",
        "\n",
        "fraud = credit[credit['Class']==1].loc[credit.duplicated()]\n",
        "print('Fraudes repetidas: {} '.format(len(fraud)))\n",
        "print('\\n')\n",
        "fraud"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnbKsZM7PtQq",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "Houve <b> 19 fraudes repetidas </b> neste dois dias, tem casos que houve três fraudes com o mesmo valor, o restante não passa de dois, algo que chama a atenção são os valores baixos das transações que houve um maior número de fraudes, talvez por que o fraudador considera-se o risco de ser pego comparado com o número de tentativas. \n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgptccMacYYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cmap\n",
        "cmap = sns.diverging_palette(120, 40, sep=20, as_cmap=True, center='dark')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HuYxMKh-Gda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Correlações\n",
        "corr = credit.corr(method='pearson')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(23,15))\n",
        "\n",
        "# cmap=Greys\n",
        "\n",
        "plt.title('Matriz de Correlação', fontsize=16)\n",
        "print('\\n')\n",
        "correlacao = sns.heatmap(corr, annot=True, cmap='Blues', ax=ax, lw=3.3, linecolor='lightgray')\n",
        "correlacao"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xi4nOSFGbWPo",
        "colab_type": "text"
      },
      "source": [
        "Olhando as correlações de Pearson, conseguimos ver que não há correlações positivasgrandes,apenas algumas features passam de <b> 0.30 de correlação, que são <b> V7 com Amount | V20 com Amount </b>.\n",
        "\n",
        "(Vamos olhar posteriormente as correlações) \n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucuNLjg2BMZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Distribuições \n",
        "\n",
        "# definindo plot \n",
        "cols_names = credit.drop(['Class', 'Amount', 'Time'], axis=1)\n",
        "idx = 0\n",
        "\n",
        "# Separando classes\n",
        "fraud = credit[credit['Class']==1]\n",
        "normal = credit[credit['Class']==0]\n",
        "\n",
        "# figura do plot \n",
        "fig, ax = plt.subplots(nrows=7, ncols=4, figsize=(18,18))\n",
        "fig.subplots_adjust(hspace=1, wspace=1)\n",
        "\n",
        "for col in cols_names:\n",
        "    idx += 1\n",
        "    plt.subplot(7, 4, idx)\n",
        "    sns.kdeplot(fraud[col], label=\"Normal\", color='blue', shade=True)\n",
        "    sns.kdeplot(normal[col], label=\"Fraud\", color='red', shade=True)\n",
        "    plt.title(col, fontsize=11)\n",
        "    plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VWRY5gGhTiX",
        "colab_type": "text"
      },
      "source": [
        "Todas as distribuições das variáveis que possuem uma \"máscara\", não sabemos a representação real dessas variáveis pois elas foram ocultadas, mas através da distribuição com a Densidade de kernel, pode ver muito bem as curvas de cada uma, comparando com uma transação normal ou Fraude. \n",
        "\n",
        "\n",
        "\n",
        "Fraude: Algumas se aproximam de uma Distribuição Gaussiana, com uma pico maior e uma cauda um bem extensa, possivelmente temos uma variação maior dos dados, nestas features.\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soo21qi9dxHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Intervalor de valores das Fraudes \n",
        "credit[credit['Class']==1]['Amount'].value_counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaWcf2F6lRoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(14,8))\n",
        "sns.boxplot(x='Class', y='Amount', data=credit, palette='coolwarm', order=credit.loc[300:320, 'Amount'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgYeRkxbmyb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Qual o valor médio da transação fraudulenta ? \n",
        "print('Valor médio Fraude: {} | Valor médio Normal: {}'.format(credit[credit['Class']==1]['Amount'].mean() , credit[credit['Class']==0]['Amount'].mean()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_wZf3vhnJ6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Qual o maior valor de fraude ?\n",
        "print('Maior valor Fraude: {}  | Maior valor Normal: {}'.format(credit[credit['Class']==1]['Amount'].max(), credit[credit['Class']==0]['Amount'].max()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1q4l1Wdnuuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.title('Valor de Transações por Classe', fontsize=15)\n",
        "sns.barplot(x='Class', y='Amount', data=credit, palette='GnBu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PZLKIWlok9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Distribuição de valores das Transações \n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title('Transações', fontsize=14)\n",
        "plt.grid(False)\n",
        "sns.kdeplot(credit['Amount'], color='lightblue', shade=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8tDLqwdruU3",
        "colab_type": "text"
      },
      "source": [
        "### Resumo de Análise\n",
        "\n",
        "Após esta etapa de exploração dos dados, conseguimos ter insights bons sobre o conjunto, transações fraudulentas tendem a ter valores menores do que transações normais, fraudes repetidas contém valores menores, outro ponto que descobrimos é que o tempo, entre uma fraude e outra, como os dados nos dizem sobre apenas dois dias de transações, fica díficil inferirmos um valor pontual, em que o fraudador faz uma transferência, mas o tempo possiu uma variação maior que transações normais.\n",
        "\n",
        "Vamos criar diversos experimentos, em busca de um modelo robusto e que atinja o nosso objetivo. \n",
        "\n",
        "<br>\n",
        "<hr>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMcsyXCZutL0",
        "colab_type": "text"
      },
      "source": [
        "### Baseline \n",
        "\n",
        "Vou criar um modelo base, de forma que eu possa comparar os próximos resultados de outros experimentos, com este modelo puro e simples que vamos construir. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ocs7TT5mu5FX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Separando feature| classe \n",
        "\n",
        "X = credit.drop('Class', axis=1)\n",
        "y = credit['Class']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30, random_state=42)\n",
        "\n",
        "\n",
        "# StandardScaler \n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyBhkoSMvbu5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modelo Baseline\n",
        "\n",
        "baseline = LogisticRegression(random_state=42)\n",
        "baseline.fit(X_train, y_train)\n",
        "y_baseline = baseline.predict(X_test)\n",
        "\n",
        "# Probabilidades \n",
        "y_proba_baseline = baseline.predict_proba(X_test)[:,1]\n",
        "\n",
        "print(classification_report(y_test, y_baseline))\n",
        "print('\\n')\n",
        "print('AUC: {}%'.format(roc_auc_score(y_test, y_proba_baseline)))\n",
        "print('Precision-Recall: {}'.format(average_precision_score(y_test, y_proba_baseline)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwgjWGmByYfj",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "\n",
        "### Baseline\n",
        "\n",
        "* AUC: 0.97\n",
        "* AUPRC: 0.78\n",
        "* Precision: 0.88\n",
        "* Recall: 0.63\n",
        "\n",
        "Um modelo baseline puro nos trouxe uma idéia do quanto podemos melhorar o modelo base, para um modelo melhor, vamos criar alguns experimentos para obter uma performance melhor. \n",
        "\n",
        "<hr>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbIsFnBT_W0G",
        "colab_type": "text"
      },
      "source": [
        "### Experimentação \n",
        "\n",
        "Vamos testar alguns algoritmos como: \n",
        "\n",
        "* Random Forest \n",
        "* SVM \n",
        "\n",
        "Vamos aplica-los separadamente, mas vamos tentar outras alternativas na modelagem, aplicar técnicas de balanceamento dos dados, e também validar nossos modelos apartir de Cross validation. \n",
        "\n",
        "<br>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1Gpqod4jRT6",
        "colab_type": "text"
      },
      "source": [
        "Vamos aplicar primeiro uma Random Forest, combinada com uma técnica de OverSampling que basicamente vai, criar dados sintéticos na class minonitária que é a classe fraude, vamos testar esta abordagem e ver a performance do modelo. Assim por diante, vamos aplicar UnderSampling também para medir a performance em uma outra abordagem. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xH-EC92kKMY",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<p align=center>\n",
        "<img src=\"https://blog.strands.com/hs-fs/hubfs/Screenshot%202019-07-18%20at%2014.15.15.png?width=600&name=Screenshot%202019-07-18%20at%2014.15.15.png\" width=\"60%\"></p>\n",
        "\n",
        "\n",
        "<br>\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_gA11ysvO6L",
        "colab_type": "text"
      },
      "source": [
        "### Random Forest \n",
        "\n",
        "Vamos construir um modelo Random Forest como <b> 200 árvores </b> combinado com a técnica de OverSampling <b> SMOTE </b> e ver os resultados das métricas que estamos buscando otimizar que são: \n",
        "\n",
        "* AUC \n",
        "* Precision\n",
        "* Recall \n",
        "* AUPRC  \n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIjWjrmO3Dj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = credit.drop('Class', axis=1)\n",
        "y = credit['Class']\n",
        "\n",
        "# Validação \n",
        "KFold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "fold = 0\n",
        "for train_index, test_index in KFold.split(X,y):\n",
        "      fold += 1 \n",
        "      print('Fold: ', fold)\n",
        "      print('Treino: ',train_index.shape[0])\n",
        "      print('Teste: ', test_index[0])\n",
        "\n",
        "      # Split\n",
        "      X = credit.drop('Class', axis=1)\n",
        "      y = credit['Class']\n",
        "\n",
        "      # OverSampling SMOTE \n",
        "      smote = SMOTE(random_state=42)\n",
        "      X, y = smote.fit_sample(X, y)\n",
        "      print('Normal: {}  |  Fraude: {}'.format(np.bincount(y)[0], np.bincount(y)[1]))\n",
        "\n",
        "      # separando dados \n",
        "      X_train, X_test = X[train_index], X[test_index]\n",
        "      y_train, y_test = y[train_index], y[test_index] \n",
        "\n",
        "      \n",
        "      # pré-processamento \n",
        "      scaler = QuantileTransformer(random_state=42)\n",
        "      X_train = scaler.fit_transform(X_train)\n",
        "      X_test = scaler.transform(X_test)\n",
        "\n",
        "      # Criando modelo \n",
        "      forest = RandomForestClassifier(n_estimators=200, max_depth=13, min_samples_split=9,\n",
        "                                    random_state=42)\n",
        "      forest.fit(X_train, y_train)\n",
        "      y_pred_forest = forest.predict(X_test)\n",
        "      y_proba_forest = forest.predict_proba(X_test)[:,1]\n",
        "\n",
        "\n",
        "      # Métricas\n",
        "      print('\\n')\n",
        "      print(classification_report(y_test, y_pred_forest))\n",
        "      print('--------------'*5)\n",
        "      print('\\n')\n",
        "      auc_forest = roc_auc_score(y_test, y_proba_forest)\n",
        "      precision_forest = precision_score(y_test, y_pred_forest)\n",
        "      recall_forest = recall_score(y_test, y_pred_forest)\n",
        "      auprc_forest = average_precision_score(y_test, y_proba_forest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trQxHermoUWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Validação Random Forest \n",
        "print('Random Forest')\n",
        "print('\\n')\n",
        "\n",
        "print('AUC: ', np.mean(auc_forest))\n",
        "print('Precision: ', np.mean(precision_forest))\n",
        "print('Recall: ', np.mean(recall_forest))\n",
        "print('Precision-Recall: ', np.mean(auprc_forest))\n",
        "\n",
        "\n",
        "print('\\n')\n",
        "print('\\n')\n",
        "\n",
        "# Curva ROC random forest \n",
        "auc_forest = np.mean(auc_forest)\n",
        "fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_test, y_proba_forest)\n",
        "\n",
        "# plot \n",
        "plt.figure(figsize=(12,7))\n",
        "plt.plot(fpr_forest, tpr_forest, color='blue', label='AUC: {}'.format(auc_forest))\n",
        "plt.fill_between(fpr_forest, tpr_forest, color='skyblue', alpha=0.3)\n",
        "plt.plot([0,1], [0,1], color='black', ls='--', label='Reference line')\n",
        "plt.xlabel('False Positive Rate', fontsize=14)\n",
        "plt.ylabel('True Positive Rate', fontsize=14)\n",
        "plt.title('ROC Random Forest', fontsize=16)\n",
        "plt.legend(loc=4, fontsize=14)\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XWrCvD7CZdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Precision-Recall Random Forest \n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "viz = PrecisionRecallCurve(forest)\n",
        "viz.fit(X_train, y_train)\n",
        "viz.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krL1G4kIHyM9",
        "colab_type": "text"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Resumo Random Forest \n",
        "\n",
        "O modelo com <b> 200 árvores </b>, combinado com um método de pré-processamento chamado <b> QuantileTransformer </b> que tem o objetivo, de reduzir o impacto de posssíveis Outliers nos dados, ele vai aproximar a distriuição das features através do IQR (intervalo interquartil), também fiz uso da técnica <b> SMOTE </b> que cria dados sintéticos, da classe minonitária igualando com a classe majoritária. \n",
        "\n",
        "<br>\n",
        "\n",
        "Em resumo o modelo não alcançou um resultado satisfatório, o AUC do modelo ficou alto, na validação o resultado foi de <b> 97%  </b>, mas as outras métricas não atingiram um valor esperado. Precision ficou alta mas o Recall não chegou a subir muito comparado a baseline, o trade-off Precision-Recall é a métrica principal em nossa avaliação, e díficil de otimizar, pois queremos um modelo que consiga separar bem uma fraude de uma transação normal, e também que identifique bem uma fraude quando ela realmente ocorrer. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiEPhrKJDsLH",
        "colab_type": "text"
      },
      "source": [
        "<hr>\n",
        "<br>\n",
        "<hr>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaobQ4niDwDj",
        "colab_type": "text"
      },
      "source": [
        "### SVM \n",
        "\n",
        "Vamos aplicar o SVM que é outro algoritmo robusto, vamos combinar nele um outro pré-processamento nos dados, vamos utilizar o UnderSampling agora, onde vamos diminuir a classe majoritária, na tentativa de igualar as classes.\n",
        "\n",
        "SVM tem a função de kernel, que permite construir um Hiperplano que melhor se adeque aos dados. \n",
        "\n",
        "<br>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSw_b7WiEPWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SVM \n",
        "X = credit.drop('Class', axis=1)\n",
        "y = credit['Class']\n",
        "\n",
        "# UnderSampling  \n",
        "under = NearMiss()\n",
        "X, y = under.fit_sample(X, y)\n",
        "\n",
        "\n",
        "# Validação \n",
        "KFold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "fold = 0\n",
        "for train_index, test_index in KFold.split(X,y):\n",
        "      fold += 1 \n",
        "      print('Fold: ', fold)\n",
        "      print('Treino: ', train_index.shape[0])\n",
        "      print('Teste: ', test_index[0])\n",
        "\n",
        "      # Classe balanceadas\n",
        "      print('Normal: {}  |  Fraude {}'.format(np.bincount(y)[0], np.bincount(y)[1]))\n",
        "\n",
        "      # separando dados \n",
        "      X_train, X_test = X[train_index], X[test_index]\n",
        "      y_train, y_test = y[train_index], y[test_index] \n",
        "\n",
        "      \n",
        "      # pré-processamento \n",
        "      scaler = RobustScaler()\n",
        "      X_train = scaler.fit_transform(X_train)\n",
        "      X_test = scaler.transform(X_test)\n",
        "\n",
        "      # Criando modelo \n",
        "      svm = SVC(C=1.0, gamma=0.5, random_state=42, probability=True)\n",
        "      svm.fit(X_train, y_train)\n",
        "      y_pred_svm = svm.predict(X_test)\n",
        "      y_proba_svm = svm.predict_proba(X_test)[:,1]\n",
        "\n",
        "\n",
        "      # Métricas\n",
        "      print('\\n')\n",
        "      print(classification_report(y_test, y_pred_svm))\n",
        "      print('-------------'*5)\n",
        "      print('\\n')\n",
        "      auc_svm = roc_auc_score(y_test, y_proba_svm)\n",
        "      precision_svm = precision_score(y_test, y_pred_svm)\n",
        "      recall_svm = recall_score(y_test, y_pred_svm)\n",
        "      auprc_svm = average_precision_score(y_test, y_proba_svm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O72kIop4GRyY",
        "colab": {}
      },
      "source": [
        "# Validação SVM \n",
        "print('SVM')\n",
        "print('\\n')\n",
        "\n",
        "print('AUC: ', np.mean(auc_svm))\n",
        "print('Precision: ', np.mean(precision_svm))\n",
        "print('Recall: ', np.mean(recall_svm))\n",
        "print('Precision-Recall: ', np.mean(auprc_svm))\n",
        "\n",
        "\n",
        "print('\\n')\n",
        "print('\\n')\n",
        "\n",
        "# Curva ROC random forest \n",
        "auc_svm = np.mean(auc_svm)\n",
        "fpr_svm, tpr_svm, thresholds_svm = roc_curve(y_test, y_proba_svm)\n",
        "\n",
        "# plot \n",
        "plt.figure(figsize=(12,7))\n",
        "plt.plot(fpr_svm, tpr_svm, color='blue', label='AUC: {}'.format(auc_svm))\n",
        "plt.fill_between(fpr_svm, tpr_svm, color='skyblue', alpha=0.3)\n",
        "plt.plot([0,1], [0,1], color='black', ls='--', label='Reference line')\n",
        "plt.xlabel('False Positive Rate', fontsize=14)\n",
        "plt.ylabel('True Positive Rate', fontsize=14)\n",
        "plt.title('ROC SVM', fontsize=16)\n",
        "plt.legend(loc=4, fontsize=14)\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbxyO0VHeg-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Precision-Recall SVM \n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title('Precision-Recall', fontsize=16)\n",
        "viz = PrecisionRecallCurve(svm)\n",
        "viz.fit(X_train, y_train)\n",
        "viz.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S-GxMdbHh8V",
        "colab_type": "text"
      },
      "source": [
        "<hr>\n",
        "<br>\n",
        "\n",
        "### Resumo SVM \n",
        "\n",
        "Com SVM conseguimos melhorar nosso Recall para: <b> 95% </b>, consequentemente nossa Precision caiu, mas em geral o desempenho deste modelo foi superior a Random Forest, AUC foi a única com um descréscimom, Precision-Recall: <b> 96% </b> foi um resultado muito bom. Combinando o SVM com  a técnica de UnderSampling obtivemos resultados bons neste experimento.\n",
        "\n",
        "Vou criar mais dois modelos utilizando dois algoritmos de <b> Gradient Boosting </b> para ver se conseguimos, resultados ainda melhores que o SVM. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTJWCHJKgEMJ",
        "colab_type": "text"
      },
      "source": [
        "<hr>\n",
        "<br>\n",
        "<hr>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvBzFCDWgIaM",
        "colab_type": "text"
      },
      "source": [
        "### XGboost \n",
        "\n",
        "O XGBoost é um algoritmo de Machine learning, baseado em árvore de decisão e que utiliza uma estrutura de Gradient boosting.\n",
        "\n",
        "Em problemas de previsão envolvendo dados não estruturados, como imagens, textos e vídeos, as redes neurais artificiais tendem a superar todos os outros algoritmos ou frameworks.\n",
        "\n",
        "No entanto, quando se trata de dados estruturados/tabulares, algoritmos baseados em árvore de decisão são considerados os melhores da sua classe no momento.\n",
        "\n",
        "Vamos criar um modelo com o XGBoost. \n",
        "\n",
        "\n",
        "(Em um próximo projeto, vou explicar o funcionamento do XGboost por inteiro) \n",
        "\n",
        "<p align=center>\n",
        "<img src=\"https://pythonawesome.com/content/images/2018/06/xgboost.png\" width=\"60%\"></p>\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJIyIfbghLJD",
        "colab_type": "text"
      },
      "source": [
        "### XGboost + SMOTE   \n",
        "\n",
        "Vamos aplicar agora a técnica de OverSampling com o XGboost, vou aderir um experimento com o <b> SMOTE </b> que será responsável por aplicar o OverSampling, vou igualar as classes maximizando as proporções da classe 1.\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck6rO9RngNpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = credit.drop('Class', axis=1)\n",
        "y = credit['Class']\n",
        "\n",
        "\n",
        "# Validação \n",
        "KFold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        " \n",
        "precision_xgboost = []\n",
        "recall_xgboost = []\n",
        "auc_xgboost = []\n",
        "precision_recall_xgboost = []\n",
        "\n",
        "\n",
        "fold = 0\n",
        "for train_index, test_index in KFold.split(X,y):\n",
        "      fold += 1 \n",
        "      print('Fold: ', fold)\n",
        "      print('Treino: ',train_index.shape[0])\n",
        "      print('Teste: ', test_index[0])\n",
        "\n",
        "      # OverSampling SMOTE \n",
        "      smt = SMOTE(random_state=42)\n",
        "      X, y = smt.fit_sample(X, y)\n",
        "      print('Normal: {}  |  Fraude: {}'.format(np.bincount(y)[0], np.bincount(y)[1]))\n",
        "\n",
        "      # separando dados \n",
        "      X_train, X_test = X[train_index], X[test_index]\n",
        "      y_train, y_test = y[train_index], y[test_index] \n",
        "\n",
        "      \n",
        "      # pré-processamento \n",
        "      scaler = QuantileTransformer()\n",
        "      X_train = scaler.fit_transform(X_train)\n",
        "      X_test = scaler.transform(X_test)\n",
        "\n",
        "      # XGboost \n",
        "      xgb = XGBClassifier(n_estimators=300, max_delta_step=1 ,eval_metric='aucpr', \n",
        "                          cpu_history='gpu', random_state=42)\n",
        "      xgb.fit(X_train, y_train)\n",
        "      y_pred = xgb.predict(X_test)\n",
        "  \n",
        "\n",
        "      # Métricas\n",
        "      precision_recall_xgboost = average_precision_score(y_test, y_pred)\n",
        "      precision_xgboost = precision_score(y_test, y_pred)\n",
        "      recall_xgboost = recall_score(y_test, y_pred)\n",
        "      auc_xgboost  = roc_auc_score(y_test, y_pred)\n",
        "      print('Precision-Recall: ', average_precision_score(y_test, y_pred))\n",
        "      print('\\n')\n",
        "      print('\\n')\n",
        "\n",
        "\n",
        "\n",
        "# Validação final  \n",
        "print('Precision-Recall: ', np.mean(precision_recall_xgboost))\n",
        "print('Recall: ', np.mean(recall_xgboost))\n",
        "print('Precision: ', np.mean(precision_xgboost))\n",
        "print('AUC: ', np.mean(auc_xgboost))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2_p7Tsptaxx",
        "colab_type": "text"
      },
      "source": [
        "<hr>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wNTHOWresphX",
        "colab": {}
      },
      "source": [
        "# Validação XGboost + SMOTE\n",
        "print('XGboost')\n",
        "print('\\n')\n",
        "\n",
        "print('AUC: ', np.mean(auc_xgboost))\n",
        "print('Precision: ', np.mean(precision_xgboost))\n",
        "print('Recall: ', np.mean(recall_xgboost))\n",
        "print('Precision-Recall: ', np.mean(precision_recall_xgboost))\n",
        "\n",
        "\n",
        "print('\\n')\n",
        "print('\\n')\n",
        "\n",
        "# Curva ROC random forest \n",
        "roc_auc_xgboost = np.mean(auc_xgboost)\n",
        "fpr_xgboost, tpr_xgboost, thresholds_xgboost = roc_curve(y_test, y_pred)\n",
        "\n",
        "# plot \n",
        "plt.figure(figsize=(12,7))\n",
        "plt.plot(fpr_xgboost, tpr_xgboost, color='blue', label='AUC: {}'.format(roc_auc_xgboost))\n",
        "plt.fill_between(fpr_xgboost, tpr_xgboost, color='skyblue', alpha=0.3)\n",
        "plt.plot([0,1], [0,1], color='black', ls='--', label='Reference line')\n",
        "plt.xlabel('False Positive Rate', fontsize=14)\n",
        "plt.ylabel('True Positive Rate', fontsize=14)\n",
        "plt.title('ROC XGboost', fontsize=16)\n",
        "plt.legend(loc=4, fontsize=14)\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fui89DZ1tYWX",
        "colab_type": "text"
      },
      "source": [
        "<hr>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM-sQ5FGtgSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Precision-Recall XGboost + SMOTE \n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title('Precision-Recall', fontsize=16)\n",
        "viz = PrecisionRecallCurve(XGBClassifier(n_estimators=300, max_delta_step=1 ,eval_metric='aucpr', \n",
        "                          cpu_history='gpu', random_state=42))\n",
        "viz.fit(X_train, y_train)\n",
        "viz.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHqvR0kkoZ_w",
        "colab_type": "text"
      },
      "source": [
        "<hr>\n",
        "<br>\n",
        "\n",
        "### XGboost + NearMiss \n",
        "\n",
        "Vamos combinar o XGboost com a técnica de UnderSampling NearMiss, vou utilizar os mesmos parâmetros que o modelo anterior, neste caso vamos minimizar a classe majoritária a uma prorção igual nossa classe minonitária. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdN5mgEGoyyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = credit.drop('Class', axis=1)\n",
        "y = credit['Class']\n",
        "\n",
        "# UnderSampling NearMiss\n",
        "under = NearMiss(random_state=42)\n",
        "X,y = under.fit_sample(X, y)\n",
        "\n",
        "# Validação \n",
        "KFold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "resultados = []\n",
        "fold = 0\n",
        "for train_index, test_index in KFold.split(X,y):\n",
        "      fold += 1 \n",
        "      print('Fold: ', fold)\n",
        "      print('Treino: ',train_index.shape[0])\n",
        "      print('Teste: ', test_index[0])\n",
        "\n",
        "      # Classes balanceadas \n",
        "      print('Normal: {} | Fraude: {}'.format(np.bincount(y)[0], np.bincount(y)[1]))\n",
        "\n",
        "\n",
        "      # separando dados \n",
        "      X_train, X_test = X[train_index], X[test_index]\n",
        "      y_train, y_test = y[train_index], y[test_index] \n",
        "\n",
        "      \n",
        "      # pré-processamento \n",
        "      scaler = StandardScaler()\n",
        "      X_train = scaler.fit_transform(X_train)\n",
        "      X_test = scaler.transform(X_test)\n",
        "\n",
        "      # XGboost \n",
        "      xgb = XGBClassifier(n_estimators=300, max_delta_step=1 ,eval_metric='aucpr', \n",
        "                          cpu_history='gpu', random_state=42)\n",
        "      xgb.fit(X_train, y_train)\n",
        "      y_pred = xgb.predict(X_test)\n",
        "\n",
        "    # Métricas\n",
        "      precision_recall_xgboost = average_precision_score(y_test, y_pred)\n",
        "      precision_xgboost = precision_score(y_test, y_pred)\n",
        "      recall_xgboost = recall_score(y_test, y_pred)\n",
        "      auc_xgboost  = roc_auc_score(y_test, y_pred)\n",
        "      print('Precision-Recall: ', average_precision_score(y_test, y_pred))\n",
        "      print('\\n')\n",
        "      print('\\n')\n",
        "\n",
        "\n",
        "\n",
        "# Validação final  \n",
        "print('Precision-Recall: ', np.mean(precision_recall_xgboost))\n",
        "print('Recall: ', np.mean(recall_xgboost))\n",
        "print('Precision: ', np.mean(precision_xgboost))\n",
        "print('AUC: ', np.mean(auc_xgboost))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2FQynSbt481",
        "colab_type": "text"
      },
      "source": [
        "<hr>\n",
        "<br>\n",
        "<hr>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87K3Fz1Ht3pI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Validação XGboost + NearMiss  \n",
        "print('XGboost')\n",
        "print('\\n')\n",
        "\n",
        "print('AUC: ', np.mean(auc_xgboost))\n",
        "print('Precision: ', np.mean(precision_xgboost))\n",
        "print('Recall: ', np.mean(recall_xgboost))\n",
        "print('Precision-Recall: ', np.mean(precision_recall_xgboost))\n",
        "\n",
        "\n",
        "print('\\n')\n",
        "print('\\n')\n",
        "\n",
        "# Curva ROC random forest \n",
        "roc_auc_xgboost = np.mean(auc_xgboost)\n",
        "fpr_xgboost, tpr_xgboost, thresholds_xgboost = roc_curve(y_test, y_pred)\n",
        "\n",
        "# plot \n",
        "plt.figure(figsize=(12,7))\n",
        "plt.plot(fpr_xgboost, tpr_xgboost, color='blue', label='AUC: {}'.format(roc_auc_xgboost))\n",
        "plt.fill_between(fpr_xgboost, tpr_xgboost, color='skyblue', alpha=0.3)\n",
        "plt.plot([0,1], [0,1], color='black', ls='--', label='Reference line')\n",
        "plt.xlabel('False Positive Rate', fontsize=14)\n",
        "plt.ylabel('True Positive Rate', fontsize=14)\n",
        "plt.title('ROC XGboost', fontsize=16)\n",
        "plt.legend(loc=4, fontsize=14)\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ujdyqlQuIza",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "<hr>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MccjWfOuLYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Precision-Recall XGboost + NearMiss \n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title('Precision-Recall', fontsize=16)\n",
        "viz = PrecisionRecallCurve(XGBClassifier(n_estimators=300, max_delta_step=1 ,eval_metric='aucpr', \n",
        "                          cpu_history='gpu', random_state=42))\n",
        "viz.fit(X_train, y_train)\n",
        "viz.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "strnS0eppbV5",
        "colab_type": "text"
      },
      "source": [
        "<hr>\n",
        "<br>\n",
        "\n",
        "### XGboost Resumo \n",
        "\n",
        "A melhor combinação até agora foi <b> XGboost + UnderSampling(NearMiss) </b> conseguimos obter valores muito bons, reduzimos nossa classe majoritária, balanceando as classes o algoritmo conseguiu generalizar melhor os dados, com uma AUPRC de <b> 94% </b> na validação, foi um resultado excelente do modelo, os dados sintéticos que o SMOTE acaba gerando, acaba não fazendo muito sentido para o algoritmo, pois são \"dados repetidos\" e que mostra, pouquíssima generalização de Fraudes. \n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "<hr>\n",
        "<br>\n",
        "<hr>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw5YtdDyqtyk",
        "colab_type": "text"
      },
      "source": [
        "### LightGBM \n",
        "\n",
        "\n",
        "<hr>\n",
        "<br>\n",
        "\n",
        "<p align=center>\n",
        "<img src=\"https://blog.mockun.com/content/images/2018/09/LightGBM.png\" width=\"60%\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqcS-a-nsEBg",
        "colab_type": "text"
      },
      "source": [
        "##### em construção................"
      ]
    }
  ]
}